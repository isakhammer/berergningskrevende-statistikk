---
title: "Exercise 1 - TMA4300 Computer Intensive Statistical Methods"
output: html_notebook
---
Question D 1 
We want to sample from 
$$
f(\theta|\mathbf{y})\propto(2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4}
$$
by using rejection sampling and using $g(\theta) = U(0,1)$ as the proposal density. We use the maximum of $f$ as the constant $c$ in rejection sampling, to ensure $\frac{f(\theta|y)}{g(\theta)} \leq c$. 

```{r}
# f is the  posterior function to sample from, not normalized
# theta: parameter between 0 and 1, sampled from U(0,1)
# y: observed values
f <- function(theta, y) {
  return((2+theta)^y[1]*(1-theta)^(y[2]+y[3])*theta^y[4])
}

# f_normalized is the posterior function, normalized
f_normalized <- function(theta, y) {
  c = integrate(f,0,1,y)$value
  return(f(theta,y)/c)
}

# alpha finds the acceptance probability alpha
alpha <- function(theta, y) {
  c = optimize(f, interval = c(0,1), maximum = TRUE, y=y)$objective
  return(f(theta, y)/c)
}

# rejection_d samples from the posterior by rejection sampling
# n: number of samples to generate
rejection_d <- function(y, n) {
  out = 1:n
  g = function() {return(runif(1))}
  
  for (i in 1:n) {
    finished = 0
    while(finished==0) {
      u = runif(1)
      theta = g()
      alpha = alpha(theta, y)
      if(u <= alpha) {
        out[i] = theta
        finished = 1
      }
    }
  }
  return(out)
}

y = c(125,18,20,34)
n = 10000

library(ggplot2)
samples_d1 <- rejection_d(y, n)
x <- seq(0,1,0.001)
data_d1 <- data.frame("samples_d1"=samples_d1)
ggplot(data=data_d1, aes(data_d1$samples_d1)) +
geom_histogram(aes(y=..density..), breaks = seq(0, 1, by = 0.005)) +theme(plot.title = element_text(hjust = 0.5))+
xlim(c(0.4,0.85)) + labs(title="Posterior density", x="x", y="Density") +
stat_function(fun = f_normalized, args = list(y), color = "red")
```
From the plot we see that the theoretical and the simulated distribution follow each other. 

We then use Monte Carlo integration with $M = 10000$ samples to estimate the posterior mean of $\theta$, that is, the integral $\int\theta f(\theta|y)d\theta$ approximated by $\frac{1}{N}\sum_{i=1}^N\theta_i$. 

```{r}
M = 10000
# 
# post_mean_mc: the estimated posterior mean
thetas <- rejection_d(y, M)
post_mean_mc <- mean(thetas)

# inside_integral is what we want to integrate
inside_integral = function(theta,y) {
    return(theta*f_normalized(theta,y))
}

# numerical_mean is the mean found by numerical integration
numerical_mean <- integrate(inside_integral,0,1,y)[[1]]
```
The estimate of the mean of $\theta$ found by Monte Carlo integration is `r post_mean_mc`. The estimate of the mean found by numerical integration is `r numerical_mean`. The two values are very similar. We plot the distribution of the simulated values of $\theta$ together with the theoretical distribution and the Monte Carlo estimate.

```{r}
x <- seq(0,1,0.001)
data_d1 <- data.frame("thetas"=thetas)
ggplot(data=data_d1, aes(data_d1$thetas)) +
geom_histogram(aes(y=..density..), breaks = seq(0, 1, by = 0.005)) +theme(plot.title = element_text(hjust = 0.5))+
xlim(c(0.4,0.85)) + labs(title="Posterior density with posterior mean", x="x", y="Density") +
stat_function(fun = f_normalized, args = list(y), color = "red")+geom_vline(xintercept = post_mean_mc,color="red")
```

Question D 3
The overall acceptance probability is 
$$
P(U\leq\frac{f(\theta)}{cg(\theta)}) = \int_{-\infty}^{\infty}\frac{f(\theta)}{cg(\theta)}g(\theta)d\theta = \int_{-\infty}^{\infty}\frac{f(\theta)}{c}d\theta = \frac{1}{c}
$$
which implies that the number of trials to simulate one sample is $c$.

We want to count how many trials we need to simulate one sample, and we check it with the theoretical value of c. We just copy the algorithm from before and add a counter. 

```{r}
# rejection_d_count samples from the posterior by rejection sampling and counts the number of trials to one acceptance
# n: number of samples to generate
rejection_d_count <- function(y, n) {
  out = 1:n
  g = function() {return(runif(1))}
  
  for (i in 1:n) {
    finished = 0
    count = 0
    while(finished==0) {
      u = runif(1)
      theta = g()
      alpha = alpha(theta, y)
      count = count + 1
      if(u <= alpha) {
        out[i] = count
        count = 0
        finished = 1
      }
    }
  }
  return(out)
}

y = c(125,18,20,34)
n = 10000

# simulated number of trials 
number_of_trials <- mean(rejection_d_count(y,n))

#c_const finds the constant c, i.e. the numerical computation of the theoretical number of trials for one acceptance
c_const <- function(y) {
  c = optimize(f_normalized, interval = c(0,1), maximum = TRUE, y=y)$objective
  return(c)
}

c_numerical <- c_const(y)
```
The number of trials in the simulation until one acceptance is `r number_of_trials`, and the numerical calcualtion of the theoretical number of trials is `r c_numerical`.

Question D 4
We want to use the Beta(1,5) distribution as the prior, instead of the U(0,1) distribution and importance sampling with self-normalizing weights. The new prior with a Beta(1,5) distribution is: 
$$
f(\theta) = \frac{\Gamma(6)}{\Gamma(5)}(1-\theta)^4=5(1-\theta)^4
$$
And the new posterior destribution becomes
$$
f_{new}(\theta|\mathbf{y}) \propto f(\theta)f(\mathbf{y}|\theta) = 5(1-\theta)^4(2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4}
$$
We use importance sampling with self-normalizing weights, such that the weights become 
$$
w(\theta_i) = \frac{f_{new}(\theta_i|\bf{y})}{f(\theta_i|\bf{y})}
$$
and because our estimate is based on the samples of $\theta$ in part D 2, we use the unnormalized $f(\theta|\bf{y})$ as $g(\theta)$ from part D 2 in the expression for the weights $w(\theta_i)$. The estimate of the mean is then 
$$
\mu = \frac{\sum_{i=1}^N\theta_iw(\theta_i)}{\sum_{i=1}^Nw(\theta_i)}
$$
which is biased, but for $n\to\inf$ it becomes approximately unbiased. 

```{r}
#fnew calculates the unnormalized new posterior distribution
fnew <- function(theta, y) {
  fnew = 5*(1-theta)^4*(2+theta)^y[1]*(1-theta)^(y[2]+y[3])*theta^y[4]
}

# weight calculates the self-normalizing weights to use in importance sampling
weight <- function(theta, y) {
  return(fnew(theta, y)/f(theta, y))
}

# fnew_normalized calculates the normalized new posterior distribution
fnew_normalized <- function(theta, y) {
  const = integrate(fnew, 0, 1, y)$value
  return(fnew(theta, y)/const)
}

fnew_normalized(0.5,y)
# importance_d4 estimates the mean via importance sampling
# y: observed values
# n: number of samples to use in the estimation
importance_d4 <- function(y, n) {
  theta = rejection_d(y, n)
  estimate = sum(theta*weight(theta, y))/sum(weight(theta,y))
  return(estimate)
}

n = 10000
simulated_d4 <- importance_d4(y, n)

# integrand_d4 is the expression to integrate
integrand_d4 <- function(theta, y) {
  return(fnew_normalized(theta, y)*theta)
}

# numerical_d4 numerically calculates the integral
numerical_d4 <- function(y) {
  return(integrate(integrand_d4, 0,1,y)$value)
}

num_d4 <- numerical_d4(y)
```
The simulated expected value is `r simulated_d4`, and the numerical value is `r num_d4`. We see that they are equal. 