---
title: "Exercise 1"
author: "Ada and Isak "
date: "2/10/2022"
output: 
   html_notebook
  #pdf_document:
  #  fig_caption: yes
---


```{r setup, include=FALSE}
## Libraries used in Project
library(ggplot2)
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```
# Problem A
## Question 1
We first want to write an R function that generates  samples from a exponential distribution with rate parameter $\lambda$. 
We know that the cumulative exponential distrobution takes the form $$ F(x;\lambda) =  1 - \lambda e ^{-\lambda x},  \quad x \ge 0 
$$

Computing the inverse cumulative function exploiting the uniform distribution $U \sim Unif(0,1)$ we can get 
$$
X = F^{-1}(u) = - ln (u)/\lambda, \quad 0 \leq u \leq 1
$$

We now want to show that this is true using simulation from the inverse cumulative distribution 

```{r figs, echo=TRUE, fig.width=4,fig.height=3,fig.cap="Analytical and inverse sampled Exponential Distribution where lambda = 1, n=60000"}
library(ggplot2)
# n: number of samples to generate
# rate: the rate
# expdist returns a vector with generated random numbers from exponential distribution

expdist <- function(rate, n) {
   u <- runif(n)
   x <-  -log( (1/rate)*( 1 - u))/rate
   list = list(x, u)
   return(list)
}

n <- 60000
lambda <- 1
outval <- expdist(lambda, n)
x = outval[[1]]
y = outval[[2]]

ggplot(data.frame(x=x))+geom_histogram(aes(x=x, y = ..density..),
bins = 50)  +geom_line(data = data.frame(x=x,y=dexp(x, rate = 1)),
aes(x=x, y=y) ,
color = "blue")+xlim(0,5)

```

From the plot one can see that the exponential distribtion that we generated and the exponential distribution in R closely follow each other, which means that our implementation is correct.


# Problem B
## Question B1
The acceptance probability has the form
$$
P(U\leq\frac{f(x)}{cg(x)}) = \int_{-\infty}^{\infty}\frac{f(x)}{cg(x)}g(x)dx = \int_{-\infty}^{\infty}\frac{f(x)}{c}dx = \frac{1}{c}
$$

where $c$ is such that $\frac{f(x)}{g(x)}\leq c$. To find $c$, consider the expression $f(x)\leq cg(x)$. Then we get (for the nontrivial cases when $x>0$)

$$
\frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}\leq \begin{cases}
c\frac{e\alpha}{e+\alpha}x^{\alpha-1}, 0 < x < 1\\
c\frac{e\alpha}{e+\alpha}e^{-x}, x \geq 1\\
\end{cases}
$$
Rearranging the inequalities and using that $e^{-x}$ and $x^{\alpha-1}$ are decreasing functions, we get the following upper bound for $c$:

$$
c\geq\frac{1}{\Gamma(\alpha)}\frac{e+\alpha}{e\alpha}
$$
So we choose $c = \frac{1}{\Gamma(\alpha)}\frac{e+\alpha}{e\alpha}$ to satisfy our initial inequality. The acceptance probability is then $\frac{1}{c} = \frac{\Gamma(\alpha)e\alpha}{e+\alpha}$.

## Question 1b
To generate $n$ independent samples from $f$ by rejection sampling, we need to find the expression $\alpha = \frac{1}{c}\frac{f(x)}{g(x)}$, with the proposal $g$ from problem A2 and $f$ as given in the exercise. We therefore use $c$ as found in the previous exercise and we sample from $g$ as in problem A2. 
```{r}
# f_b1 returns the value of f in this exercise, a gamma distribution
f_b1 <- function(alpha, x) {
  gamma = (1/gamma(alpha))*x^(alpha-1)*exp(-x)
  return((gamma)*(x>0))
}

# acceptance_criterion returns the alpha to accept/reject in rejection sampling
acceptance_criterion <- function(alpha, x) {
  const = (exp(1)+alpha)/(gamma(alpha)*exp(1)*alpha)
  alpha = f_b1(alpha, x)/(const*g_test(alpha, x))
  return(alpha)
}

# rejection_b generates samples from f by rejection sampling
# n: number of samples to generate
# alpha: shape parameter 
rejection_b <- function(alpha, n) {
  out = 1:n
  
  for (i in 1:n) {
    finished = 0
    while(finished==0) {
      u = runif(1)
      x = dist1(alpha, 1)[[1]]
      acceptance = acceptance_criterion(alpha, x)
      if(u <= acceptance) {
        out[i] = x
        finished = 1
      }
    }
  }
  return(out)
}


# f_testb is the theoretical function to test the algorithm with 
f_testb = function(x, alpha) {
  gamma = (1/gamma(alpha))*x^(alpha-1)*exp(-x)
  return((gamma)*(x>0))
}

n = 10000
alpha = 0.5


library(ggplot2)
samples_b1 <- rejection_b(alpha, n)
x <- seq(0,4,0.001)
data_b1 <- data.frame("samples_b1"=samples_b1)


ggplot(data=data_b1, aes(data_b1$samples_b1)) +
geom_histogram(aes(y=..density..), breaks = seq(0, 4, by = 0.1)) +theme(plot.title = element_text(hjust = 0.5))+
xlim(c(0,4)) + labs(title="Samples from gamma distribution", x="x", y="Density") +
stat_function(fun = f_testb, args = list(alpha), color = "red")
```
            

## Question B2b

We can define the domain 
$$
C_f = \Big \{ (x_1, x_2) : 0 \le x_1 \le \sqrt{f^{*}\Big(\frac{x_2}{x_1}\Big)}  \Big \}
$$
where 
$$
f(x) =
\begin{cases}
x^{\alpha -1}e^{-x}, \quad & x> 0 \\
0, \quad & x\le 0 \\
\end{cases}
$$ We now want to define some upper and lower bounds such that 
$$
\begin{cases}
% align funka ikje, sÃ¥ bruka cases
a &= \sqrt{{sup}_x f(x)  } = \sqrt{f(\alpha -1)} \\ 
b_{+}&= \sqrt{{sup}_{x \ge 0} x^2 f(x)  } = \sqrt{(\alpha +1)^2 f(\alpha +1)}\\
b_{-} &= \sqrt{{sup}_{x \le 0} x^2 f(x)  } = 0
\end{cases}
$$

The supremum for $a$ and $b_{+}$ can easily be derived by finding extremal points of the function using differentiation and $b_{-}$ follows from $f(x) = 0$ for $x\le 0$.


## Question B2a

We can define the domain 
$$
C_f = \Big \{ (x_1, x_2) : 0 \le x_1 \le \sqrt{f^{*}\Big(\frac{x_2}{x_1}\Big)}  \Big \}
$$
where 
$$
f(x) =
\begin{cases}
x^{\alpha -1}e^{-x}, \quad & x> 0 \\
0, \quad & x\le 0 \\
\end{cases}
$$ We now want to define some upper and lower bounds such that 
$$
\begin{align}
a &= \sqrt{{sup}_x f(x)  } = \sqrt{f(\alpha -1)} \\ 
b_{+} &= \sqrt{{sup}_{x \ge 0} x^2 f(x)  } = \sqrt{(\alpha +1)^2 f(\alpha +1)}\\
b_{-} &= \sqrt{{sup}_{x \le 0} x^2 f(x)  } = 0
\end{align}
$$

The supremum for $a$ and $b_{+}$ can easily be derived by finding extremal points of the function using differentiation and $b_{-}$ follows from $f(x) = 0$ for $x\le 0$.

## Question B2b
To sample from the gamma distribution for large values of $\alpha$, we use the ratio of uniforms method with the values of $a, b_{-}$ and $b_{+}$ as above. To avoid computational errors following large numbers, we use a log transform. To sample from the uniform distribution between 0 and $a_-$ and between 0 and $b_+$, we use that 

$$
x_1 = a\cdot u_1\\
x_2 = b_+\cdot u_2
$$
which is a transform from the standard uniform variables $u_1, u_2$ to a uniform distribution between 0 and some other limit. In the log scales, this becomes

$$
\log(x_1) = \log(a) + \log(u_1)\\
\log(x_2) = \log(b_+) + \log(u_2)
$$
The acceptance criterion is then 

$$
x_1 \leq \sqrt{\frac{x_2^{\alpha-1}e^{-x_2}}{x_1^{\alpha-1}e^{-x_2}}}
$$
and on the log scale we get
$$
\log(x_1) \leq \frac{1}{2}(\alpha-1)(\log(x_2)-\log(x_1))-e^{\log(x_2)-log(x_1)}
$$

```{r}
n <- 10000
# f_gamma the kernel of a gamma function
f_gamma <- function(x, alpha){
  return (x^(alpha - 1 )*exp(-x)*(x>0))
}

alpha <- 400

# gamma_b2b2 samples from gamma distribution with large alpha
gamma_b2b2 <- function(alpha, n){
  x = 1:n
  loga = 0.5*((alpha-1)*log(alpha-1)+(1-alpha))
  logb = 0.5*((alpha+1)*log(alpha+1)-1-alpha)
  trials = 1
  
  for (i in 1:n){
    logx1 = loga + log(runif(1))
    logx2 = logb + log(runif(1))
    f_condition = 0.5*((alpha-1)*(logx2-logx1)-exp(logx2-logx1))
    while(logx1 > f_condition){
    logx1 = loga + log(runif(1))
    logx2 = logb + log(runif(1))
    f_condition = 0.5*((alpha-1)*(logx2-logx1)-exp(logx2-logx1))
    trials = trials + 1
    }
    
     x[i] = exp(logx2-logx1)
  }
  return(cbind(x, trials-1))
}


samples_b2b = gamma_b2b2(alpha, n)[,1]
hist(samples_b2b, breaks = 40, freq = FALSE, main = bquote("Histogram of gamma distribution when "~alpha == .(alpha)))
curve(dgamma(x, alpha, 1), col = "red", lwd = 2, add=TRUE)

test_tries <- function(n) {
  alpha = seq(2,2000,10)
  tryvector = 1:length(alpha)
  for (i in 1:length(alpha)) {
    tries = gamma_b2b2(alpha[i], n)[,2][1]
    tryvector[i] = tries
  }
  return(cbind(alpha, tryvector))
  
}

test_try = test_tries(1000)

plot(test_try[,1], test_try[,2])

```
From the plot we see that a larger value of $\alpha$ implies that we need more tries to accept one sample. This makes sense, as we are approaching a more extreme case when $\alpha$ becomes large. 


## Question B3
We want to generate $n$ samples of the gamma distribution

$$
f(x) = \frac{\beta^{\alpha}}{\Gamma (\alpha)} x^{\alpha -1} e^{-\beta}
$$

for $\alpha > 0$ and $\beta > 0$. We use that $Y = \frac{X}{\beta}$ is Gamma($\alpha$, $\beta$) distributed if $X$ is Gamma($\alpha$) distributed. To show this, use the moment generating function of $Y$ and the properties of moment generating functions. 

$$
M_Y(t) = M_X\bigg(\frac{t}{\beta}\bigg) = \bigg(1-\frac{t}{\beta}\bigg)^{-\alpha}
$$
which we recognize as the MGF of a Gamma($\alpha$, $\beta$) variable. When $\alpha = 1$, we recognize this as the MGF of an Exponential($\beta$) distribution, so therefore we can sample from the exponential distribution in this case. We use the rejection sampling algorithm for $\alpha \in (0,1)$ and the ratio of uniforms algorithm for $\alpha>1$. 

```{r}
# Delete this before merging the files, only for help
# n: number of samples to generate
# rate: the rate
# expdist returns a vector with generated random numbers from exponential distribution
expdist <- function(rate, n) {
   u <- runif(n)
   x <-  -log( (1/rate)*( 1 - u))/rate
   list = list(x, u)
   return(list)
}

# gammaab samples from the gamma distribution for all values of alpha and beta > 0 
gammaab <- function(alpha, beta, n) {
  if(alpha>0&&alpha<1) {
    X = rejection_b(alpha, n)/beta
  }
  else if(alpha==1) {
    X = expdist(1, n)[[1]]
  }
  else if (alpha>1) {
    X = gamma_b2b2(alpha,n)[,1]
  }
  else {
    print("alpha not valid")
    return(0)
  }
  Y = X/beta
  return(Y)
}

# theoretical_gamma is the theoretical function to test the algorithm with for alpha, beta > 0
theoretical_gamma = function(x, alpha, beta) {
  gamma = ((beta^alpha)/gamma(alpha))*x^(alpha-1)*exp(-beta*x)
  return((gamma)*(x>0))
}

alpha = 2
beta = 2
n = 10000

samples_ab <- gammaab(alpha, beta, n)
hist(samples_ab, breaks = 40, freq = FALSE, main = bquote("Histogram of samples from gamma when "~alpha == .(alpha) ~beta ==.(beta)))
curve(theoretical_gamma(x, alpha, beta), col = "red", lwd = 2, add=TRUE)

```

## Question B4a
We want to show that if $x \sim  \text{Gamma}(\alpha, 1)$ and $y \sim \text{Gamma}(\beta, 1)$ are independent, then $z = \frac{x}{x+y}$ is $\text{Beta}(\alpha, \beta$) distributed. Define $v = x + y$ such that $x = zv$ and $y = v(1-z)$. Then the jacobian of the transformation is $v$, so by using the transformation of variables formula, the joint distribution of $z$ and $v$ becomes 

$$
f_{zv}(z,v) = \frac{v}{\Gamma(\alpha)\Gamma(\beta)}(vz)^{\alpha-1}(v(1-z))^{\beta-1}e^{-v}
$$

which can be written as 

$$
\frac{v^{\alpha+\beta-1}}{\Gamma(\alpha)\Gamma(\beta)}z^{\alpha-1}(1-z)^{\beta-1}e^{-v}
$$
and we see that this is just a gamma distribution and a beta distribution multiplied, so $v \sim \text{Gamma}(\alpha+\beta, 1)$ and $z \sim  \text{Beta} (\alpha, \beta)$. 

## Question 4b
### TODO
To sample from a Beta($\alpha$, $\beta$ ) distribution, we use the result above. 

```{r}
# sample_beta samples from beta(alpha, beta) distribution 
# n: number of samples
sample_beta <- function(alpha, beta, n) {
  x = gammaab(alpha, 1, n)
  y = gammaab(beta, 1, n)
  z = x/(x+y)
  return(z)
}

alpha = 0.5
beta = 0.5
n = 10000
samples_beta <- sample_beta(alpha, beta, n)
hist(samples_beta, breaks = 40, freq = FALSE, main = bquote("Histogram of samples from beta when "~alpha == .(alpha) ~beta ==.(beta)))
curve(dbeta(x, alpha, beta), col = "red", lwd = 2, add=TRUE)

```


