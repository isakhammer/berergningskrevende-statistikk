---
title: "Exercise 1 - TMA4300 Computer Intensive Statistical Methods"
output: html_notebook
---
# Problem A
## Question 1
We first want to write an R function that generates  samples from a exponential distribution with rate parameter $\lambda$. 
We know that the cumulative exponential distrobution takes the form $$ F(x;\lambda) =  1 - \lambda e ^{-\lambda x},  \quad x \ge 0 
$$

Computing the inverse cumulative function exploiting the uniform distribution $U \sim Unif(0,1)$ we can get 
$$
X = F^{-1}(u) = - ln (u)/\lambda, \quad 0 \leq u \leq 1
$$

We now want to show that this is true using simulation from the inverse cumulative distribution 
```{r}
# n: number of samples to generate
# rate: the rate
# expdist returns a vector with generated random numbers from exponential distribution
expdist <- function(rate, n) {
   u <- runif(n)
   x <-  -log( (1/rate)*( 1 - u))/rate
   list = list(x, u)
   return(list)
}
n <- 60000
lambda <- 1
outval <- expdist(lambda, n)
x = outval[[1]]
y = outval[[2]]
library(ggplot2)
ggplot(data.frame(x=x))+geom_histogram(aes(x=x, y = ..density..), bins = 50)+geom_line(data = data.frame(x=x,y=dexp(x, rate = 1)), aes(x=x, y=y), color = "blue")+xlim(0,5)
```
From the plot one can see that the exponential distribtion that we generated and the exponential distribution in R closely follow each other, which means that our implementation is correct.


## Question 2 a

Let us consider a new example where the PDF has the form
$$
g(x) = 
\begin{cases}
cx^{\alpha -1},& \quad 0 < x < 1 \\
ce^{-x},& \quad x  \ge 1 \\
\end{cases}
$$

The cumulative function can be computed with integration

$$
G(x) = 
\begin{cases}
\frac{c}{\alpha}x^{\alpha},& \quad 0 < x < 1 \\
1-ce^{-x},& \quad x  \ge 1 \\
0, &\quad \text{otherwise}
\end{cases}
$$

And similarly with the inverse cumulative function

$$
G^{-1} (u) = \begin{cases}
 (\frac{u\alpha}{c})^{\frac{1}{\alpha}}, \quad 0<u < c/\alpha \\
-ln(\frac{1-u}{c}), \quad u\geq c/\alpha \\
0, \quad \text{otherwise}
\end{cases}
$$

with c as $\frac{e\alpha}{e+\alpha}$ in all the expressions, which is found by using that the density integrated over the whole space must be 1. 

## Question 2 b 

This distribution can be plotted by using the inverse cumulative distribution as before.

```{r}
# ca: normalizing constant
ca <- function(alpha) {
  return(exp(1)*alpha/(exp(1)+alpha))
}
# alpha: parameter alpha
# n: number of samples to generate
# dist1 returns a vector of random numbers from the distribution g
dist1 <- function(alpha, n) {
   u <- runif(n)
   L = u<ca(alpha)/alpha
   res = c(1:n)*0
   res[L] = (u[L]*alpha/ca(alpha))^(1/alpha)
   res[!L] = -log((1-u[!L])/ca(alpha))
   return(list(res,u))
}

g_test = function(alpha, x) {
  res = c(1:length(x))*0
  L = x<1&x>0
  res[L] = ca(alpha)*x[L]^(alpha-1)
  res[!L] = ca(alpha)*exp(-x[!L])
  return (res)
}

n <- 60000
alpha <- 0.5
outval1 <- dist1(alpha, n)
x <- outval1[[1]]
y <- outval1[[2]]

hist(x, breaks = 50, freq = FALSE, main = bquote("Histogram of samples when "~alpha == .(alpha)))
curve(g_test(alpha, x), col = "red", lwd = 2, add=TRUE)
```

Question 3 a
The normalizing constant is found by using that a PDF must integrate to 1, and therefore we get $c = \alpha$.

Question 3 b
By integrating the PDF from $-\inf$ to x, we find the CDF: 
$$ 
F(x) = \frac{-1}{1+e^{\alpha x}}+1
$$
and the inverse of F is found by solving for x (where $F(x)$ is $y$:
$$
F^{-1}(x) = \frac{1}{\alpha}ln(\frac{-y}{y-1})
$$

Question 3 c
We want to use the inversion method to generate random samples from the distribution in question 3b, and compare them with the theoretical values. 
```{r}
# alpha: the parameter alpha
# n: number of samples to be generated
# dist3 gives a vector of random variables from the distribution in question 3b
dist3 <- function(alpha, n) {
   u <- runif(n)
   x <- 1/alpha*log(-u/(u-1))
   list = list(x, u)
   return(list)
}
# checkfunc3 gives values from the theoretical function that we want to check with
x <- seq(from = -10, to = 10, length.out = 100)
checkfunc3 <- function(alpha, x) {
  return(alpha*exp(alpha*x)/(1+exp(alpha*x))^2)
}
outval3 <- dist3(alpha, n)
ggplot(data.frame(x = outval3[[1]], y = outval3[[2]]))+
  geom_histogram(aes(x=x, y=..density..),alpha=0.2)+geom_line(data = data.frame(x=x,y=checkfunc3(alpha,x)), aes(x=x, y=y), color = "blue")

```
From the plot we see that our simulation follows the theoretical values, although it could be better. 

Question 4
We want to sample from the univariate normal distribution using the Box-Müller algorithm. We compare it to the normal distribution implemented in R. 

```{r}
# n: number of samples to generate
# mynormal samples from box-muller algorithm
mynormal <- function(n) {
  u1=runif(n)
  u2=runif(n)
  return (sqrt(-2*log(u1))*sin(2*pi*u2))
}
n = 100000
mynormalout <- mynormal(n)
std_normal_data <- data.frame(x=mynormalout)
x <- seq(-4, 4, length=100)
ggplot(std_normal_data)+
  geom_histogram(aes(x=x, y=..density..),bins=50)+geom_line(data = data.frame(x=x,y=dnorm(x)), aes(x=x, y=y), color = "blue")
```

## Question 5
We want to sample from a d-variate normal distribution with mean $\mu$ and covariance matrix $\Sigma$. We use the function mynormal with the Box-Muller algorithm to construct a vector with values of $\mu$ and a positive definite matrix $\Sigma = MM^T$, where $M$ is a d by d matrix from a normal distribution. As we don’t have given values of$\mu$ and $\Sigma$, we just choose a $\mu$ and $\Sigma$ such that $\Sigma$ is positive definite by the Cholesky factorization. Note that in the code, sigma = $\mu^T\mu$ because of how the Cholesky function is implemented in R. 

```{r}
# mu: mean of the multivariate normal distribution
# sigma: covariance matrix of the multivariate normal distribution
dim = 2
mu <- c(1,2)
M=chol(matrix(c(2,1,1,3),dim,dim))
sigma <- t(M)%*%M
```

To simulate from a d-variate normal distribution, we use that $y = \mu + Mx$, where $\Sigma = MM^T$. We simulate $n$ realisations from the multivariate normal distribution below. Note that the code uses $y = \mu + M^Tx$ because of how the Cholesky function is implemented in R. 

```{r}
# y: matrix to store the realisations of the multivariate normal distribution
n = 100000
y = matrix(0, dim, n)
for (i in 1:n) {
   y[,i] = mu + t(M)%*%mynormal(dim)
}
# means: vector with the mean of the simulated values
means=numeric(dim)
for (i in 1:dim){
means[i]=mean(y[i,])
}
# covmatrix: matrix with the covariances of the simulated values
covmatrix = cov(t(y))
print("The chosen mean vector is: ")
print(mu)
print("The simulated mean vector is: ")
print(means)
print("The chosen covariance matrix is: ")
print(sigma)
print("The simulated covariance vector is: ")
print(covmatrix)
```
We see that the chosen mean and covariance is almost equal to the simulated values, which shows that the implementation is correct. 


# Problem B
## Question 1a

The acceptance probability has the form
$$
\begin{align}
P(U \le  \frac{f(y)}{cg(y)}) &=  \int_{-\infty}^{\infty} \int_0^{\frac{f(y)}{cg(y)}}  f_{U,Y} (y,u) du dy \\
&=  \int_{-\infty}^{\infty} \int_0^{\frac{f(y)}{cg(y)}} 1\cdot g(y)   du dy \\
&=  1/c
\end{align}
$$

We know from task 1 that the $g(y)$ has the form 
# Task B1a


## Task B1b
We will now demonstrate how to generate $n$ samples of $f$
```{r}
#TODO: Generate n independent samples from f
```


# Problem B
## Question 2a

We can define the domain 
$$
C_f = \Big \{ (x_1, x_2) : 0 \le x_1 \le \sqrt{f^{*}\Big(\frac{x_2}{x_1}\Big)}  \Big \}
$$
where 
$$
f(x) =
\begin{cases}
x^{\alpha -1}e^{-x}, \quad & x> 0 \\
0, \quad & x\le 0 \\
\end{cases}
$$ We now want to define some upper and lower bounds such that 
$$
\begin{align}
a_{-} &= \sqrt{{sup}_x f(x)  } = \sqrt{f(\alpha -1)} \\ 
b_{+} &= \sqrt{{sup}_{x \ge 0} x^2 f(x)  } = \sqrt{(\alpha +1)^2 f(\alpha +1)}\\
b_{-} &= \sqrt{{sup}_{x \le 0} x^2 f(x)  } = 0
\end{align}
$$

The supremum for $a_{-}$ and $b_{+}$ can easily be derived by finding extremal points of the function using derivation and $b_{-}$ follows from $f(x) = 0$ for $x\le 0$.

## Question 2b

We now want to generate $n$ samples of $f$ for different values of $alpha$ 

However, the explicit function will quickly become very large for big $\alpha$. To handle this will we introduce the substitution $x = ln(y) = g(y)$ such that $g^{-1} (x) = e^x,  g^{-1}`(x) = e^x $. Hence, using the change of variable function, we get

$$
\begin{align}
f_Y (y) &= f_X(g^{-1}(x))\cdot \left| \frac{d g^{-1}(x)} {dx} \right| \\
  &= f_X(e^x) \cdot  e^x  \quad \text{completely wrong}
\end{align}
$$

```{r}
# TODO: Write an R function that generates a vector of n independent samples from f. Use the function
# to check how many tries the algorithm needs to generate n = 1000 realisations depending on
# the value of α ∈ (1, 2000]. Generate a plot with values of α on the x-axis and the number of
# tries used on the y-axis. Interpret the result.
# Caution: You need to implement the algorithm on log-scale, otherwise you will get NAs already
# for α around 30.
```


## Question 3


We want to generate $n$ samples of the gamma distribution

$$
f(x) = \frac{\beta^{\alpha}}{\Gamma (\alpha)} x^{\alpha -1} e^{-\beta}
$$



```{r}
# TODO: Write an R function that generates a vector of n independent samples from a gamma distribution
#with parameters α and β. Note that the function should work for any values α > 0 and β > 0,

rate <- 1 
x <- seq(0, 5, by=0.0001)  
alpha =3
beta=1
plot(x,dgamma(x, shape=alpha, rate=beta))

```


# Problem C 
## Question 1
```{r}
# Delete this before the exercise is finished, only for help 
mynormal <- function(n) {
  u1=runif(n)
  u2=runif(n)
  return (sqrt(-2*log(u1))*sin(2*pi*u2))
}
```


We want to estimate $P(X>4)$ by Monte Carlo integration, so we use that we can approximate the integral $\int h(x)f(x)dx$ with the sum $\frac{1}{N}\sum_{i=1}^Nh(x_i)$. Since we only want a part of the domain, we use the indicator function $I(x>4)$ as $h(x)$. We use the Box-Muller algorithm from before to draw samples from the normal distribution. 

```{r}
# indicator_func return
indicator_func <- function(x) {
  return (x>4)
}

# monte_carlo estimates the probability and the variance
# n: number of samples to generate when estimating

monte_carlo <- function(n) {
  normal = mynormal(n)
  h = indicator_func(normal)
  theta_estimate = (1/n)*sum(h)
  variance_estimate = var(h)/(n-1)
  return(c(theta_estimate, variance_estimate))
}

n = 100000

monte_carlo_test <- monte_carlo(n)
# lower: lower limit of confidence interval
# upper: upper limit of confidence interval

lowermc <- monte_carlo_test[1] + qnorm(0.025)*sqrt(monte_carlo_test[2])

uppermc <- monte_carlo_test[1] + qnorm(0.975)*sqrt(monte_carlo_test[2])

```
<!-- The estimate of the probability $P(X>4)$ is `r monte_carlo_test[1]`. The confidence interval for $\theta$ is [`r lowermc`, `r uppermc`]. -->

## Question 2
We want to use importance sampling to estimate $P(X>4)$. To determine the normalizing constant $c$, we use that the density $g(x)$ must integrate to 1, and we get that $c = e^8$. To estimate $P(X>4)$ by importance sampling, we can estimate $\int f(x)h(x)dx$ with $\frac{1}{N}\sum_{i=1}^N\frac{f(x_i)h(x_i)}{g(x_i)}$, where $x_i$ are sampled from the distribution $g(x)$, and in our case this is done by inversion sampling. 

The function we want to sample from is 
$$
g(x)= \begin{cases} 
      cxe^{-0.5x^2} &  x > 4 \\
      0 & \text{otherwise }
   \end{cases}
$$
By integrating from $t=4$ to an arbitrary $x$, we find that the cumulative distribution is 

$$
G(x) = 1-e^{8-0.5x^2}
$$
Solving this equation for $x$ we get

$$
G^{-1}(u) = \sqrt{16-2log(1-u)}
$$
```{r}
# importance_sampling estimates the probability via the density g
# n: number of samples to generate
importance_sampling <- function(n) {
  u = runif(n)
  x = sqrt(16-2*log(1-u))
  f = 1/sqrt(2*pi)*exp(-0.5*x^2)
  g = x*exp(-0.5*x^2+8)
  h = indicator_func(x)
  theta_estimate = (1/n)*sum(h*f/g)
  variance_estimate = (1/(n*(n-1)))*sum(h*f/g-theta_estimate)^2
  return(c(theta_estimate, variance_estimate))
}

n = 10^5
importance_test <- importance_sampling(n)

# lower: lower limit of confidence interval
# upper: upper limit of confidence interval
lowerim <- importance_test[1] + qnorm(0.025)*sqrt(importance_test[2])
upperim <- importance_test[1] + qnorm(0.975)*sqrt(importance_test[2])
```
<!--
The estimate of $P(X>4)$ with importance sampling is `r importance_test[1]`. The confidence interval for $\theta$ with importance sampling is [`r lowerim`, `r upperim`]. Compared to the confidence interval for Monte Carlo integration [`r lowermc`, `r uppermc`], we see that the confidence interval from importance sampling is smaller, and therefore we prefer importance sampling over Monte Carlo integration for this problem. 
-->

We want to find how many samples we need to get the same precision for Monte Carlo integration as with importance sampling. We test with some values of $n$.
```{r}
monte_carlo_test5 <- monte_carlo(10^5)
monte_carlo_test6 <- monte_carlo(10^6)
monte_carlo_test7 <- monte_carlo(10^7)

lowermc5 <- monte_carlo_test5[1] + qnorm(0.025)*sqrt(monte_carlo_test5[2])
uppermc5 <- monte_carlo_test5[1] + qnorm(0.975)*sqrt(monte_carlo_test5[2])
lowermc6 <- monte_carlo_test6[1] + qnorm(0.025)*sqrt(monte_carlo_test6[2])
uppermc6 <- monte_carlo_test6[1] + qnorm(0.975)*sqrt(monte_carlo_test6[2])
lowermc7 <- monte_carlo_test7[1] + qnorm(0.025)*sqrt(monte_carlo_test7[2])
uppermc7 <- monte_carlo_test5[1] + qnorm(0.975)*sqrt(monte_carlo_test7[2])
```
The confidence intervals for $n = 10^5, 10^6$ and $10^7$ with Monte Carlo integration are respectively: [r lowermc5, r uppermc5], [r lowermc6, r uppermc6], and [r lowermc7, r uppermc7]. We see that the confidence interval has a similar size for Monte Carlo integration as for importance sampling for $n = 10^6$, but with $n = 10^7$ the precision is even better. 

## Question 3 a
We want to produce $n$ pairs of antithetic variates from the specified distribution. The estimate is given by averaging the samples from importance sampling. The function below also estimates $\theta$ and the variance to be used in the confidence interval. 

```{r}
# antithet gives an estimate of the probability and the variance 
# n: number of sample pairs to generate
antithet <- function(n){
  u = runif(n)
  x1 = sqrt(16-2*log(u)) 
  x2 = sqrt(16-2*log(1-u))
  f1 = (1/sqrt(2*pi))*exp(-0.5*x1^2) 
  f2 = (1/sqrt(2*pi))*exp(-0.5*x2^2)
  g1 = x1*exp(-0.5*x1^2+8) 
  g2 = x2*exp(-0.5*x2^2+8)
  h1 = indicator_func(x1)
  h2 = indicator_func(x2)
  
  theta_estimate1 = (1/n)*sum(h1*f1/g1)
  theta_estimate2 = (1/n)*sum(h2*f2/g2)
  theta_estimate = (theta_estimate1+theta_estimate2)/2
  
  variance_estimate1 = (1/(n-1))*sum(h1*f1/g1-theta_estimate1)^2
  variance_estimate2 = (1/(n-1))*sum(h2*f2/g2-theta_estimate2)^2
  cov = (1/(n-1))*sum(h1*f1/g1-theta_estimate1)*sum(h2*f2/g2-theta_estimate2)
  variance_estimate = (1/(4*n))*(variance_estimate1+variance_estimate2+2*cov)
  
  return(c(theta_estimate, variance_estimate))
}
```

## Question 3 b
Now we use the function above to estimate the probability $\theta = P(X>4)$ and the variance of $\theta$. Because the function gives pairs of random variables, we will get a similar result if we use antithetic variables with $n = 5\cdot10^4$ as if we use importance sampling with $n = 10^5$ variables. 

```{r}
antithetic_test <- antithet(5*10^4)

loweranti <- antithetic_test[1] + qnorm(0.025)*sqrt(antithetic_test[2])

upperanti <- antithetic_test[1] + qnorm(0.975)*sqrt(antithetic_test[2])
```
The estimate of the probability $P(X>4)$ with antithetic variables is r antithetic_test[1]. The confidence interv al is r loweranti, r upperanti. We see that as expected, the precision of the antithetic variables and the importance sampling is similar. We must use a value of $n$ that is half as big for antithetic variables as for importance sampling because antithetic variables produces pairs of variables, and therefore the total amount of simulated samples will be the same in the end. 